{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c37302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, textstat, torch, requests, json\n",
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (\n",
    "    AutoTokenizer, T5ForConditionalGeneration, pipeline\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79168d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE   = 0 if torch.cuda.is_available() else -1\n",
    "MAX_TOK  = 512        # CoEdit token cap\n",
    "OLL_HOST = \"http://localhost:11434\"   # default Ollama REST host\n",
    "OLL_MOD  = \"mistral:latest\"\n",
    "\n",
    "# ---------- metric objects (loaded once) -------------------\n",
    "_bleu       = evaluate.load(\"bleu\")\n",
    "_bertscore  = evaluate.load(\"bertscore\")\n",
    "_perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "_sari       = evaluate.load(\"sari\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01489a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    return [\" \".join(t.strip().split()) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45bc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "_tok_coedit = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n",
    "_mod_coedit = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"grammarly/coedit-large\"\n",
    ").to(DEVICE)\n",
    "_pipe_coedit = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=_mod_coedit,\n",
    "    tokenizer=_tok_coedit,\n",
    "    device=DEVICE,\n",
    "    do_sample=False,               # deterministic\n",
    "    max_length=MAX_TOK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875a17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coedit_single(text: str) -> str:\n",
    "    prompt = (\"Paraphrase and improve the clarity, style, and grammar \"\n",
    "              f\"of the following text: {text}\")\n",
    "    return _pipe_coedit(prompt)[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ee873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coedit_edit(texts: List[str]) -> List[str]:\n",
    "    outs = []\n",
    "    for t in texts:\n",
    "        # chunk if needed\n",
    "        cur, chunks = \"\", []\n",
    "        for s in sent_tokenize(t):\n",
    "            if _tok_coedit(cur + \" \" + s, return_tensors=\"pt\").input_ids.shape[1] < MAX_TOK:\n",
    "                cur += \" \" + s\n",
    "            else:\n",
    "                chunks.append(cur.strip()); cur = s\n",
    "        if cur: chunks.append(cur.strip())\n",
    "        fixed = \" \".join(_coedit_single(c) for c in chunks)\n",
    "        outs.append(fixed)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafa7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tag_re = re.compile(r\"<fixg>(.*?)</fixg>\", re.S)\n",
    "\n",
    "def _ollama_call(prompt: str) -> str:\n",
    "    data = {\"model\": OLL_MOD, \"stream\": False, \"prompt\": prompt}\n",
    "    r = requests.post(f\"{OLL_HOST}/api/generate\", json=data, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"]\n",
    "\n",
    "def mistral_edit(texts: List[str]) -> List[str]:\n",
    "    outs = []\n",
    "    for t in texts:\n",
    "        prm = (\"Please fix grammatical errors in this sentence and improve \"\n",
    "               f\"its style. Put the result between <fixg> and </fixg> tags.\\n\\n{t}\")\n",
    "        resp = _ollama_call(prm)\n",
    "        m = _tag_re.search(resp)\n",
    "        outs.append(m.group(1).strip() if m else resp.strip())\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6307c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre(txts):        # whitespace normalise\n",
    "    return [\" \".join(x.strip().split()) for x in txts]\n",
    "\n",
    "def evaluate_outputs(preds: List[str],\n",
    "                     sources: List[str],\n",
    "                     references: List[List[str]]) -> dict:\n",
    "    src = _pre(sources)\n",
    "    prd = _pre(preds)\n",
    "    sari  = _sari.compute(sources=src, predictions=prd, references=references)[\"sari\"]\n",
    "    bleu  = _bleu.compute(predictions=prd, references=references)[\"bleu\"]\n",
    "    bert  = _bertscore.compute(predictions=prd,\n",
    "                               references=[r[0] for r in references],\n",
    "                               lang=\"en\")[\"f1\"]\n",
    "    fkgl  = float(np.mean([textstat.flesch_kincaid_grade(p) for p in prd]))\n",
    "    fre   = float(np.mean([textstat.flesch_reading_ease(p)  for p in prd]))\n",
    "    ppl   = float(np.mean(_perplexity.compute(\n",
    "                model_id=\"gpt2\", predictions=prd)[\"perplexities\"]))\n",
    "    return dict(sari=sari, bleu=bleu,\n",
    "                bert_f1=float(np.mean(bert)),\n",
    "                fkgl=fkgl, flesch=fre, perplexity=ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ba024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- COEDIT metrics --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epicg\\anaconda3\\envs\\NLPA3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\epicg\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sari': 37.63946900280527, 'bleu': 0.022973195426686147, 'bert_f1': 0.8749582767486572, 'fkgl': 11.290000000000001, 'flesch': 53.376999999999995, 'perplexity': 43.729872703552246}\n",
      "\n",
      "-- MISTRAL metrics --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sari': 38.79894941991607, 'bleu': 0.014694292423958269, 'bert_f1': 0.8709092020988465, 'fkgl': 12.66, 'flesch': 43.81699999999999, 'perplexity': 41.56548843383789}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from datasets import load_dataset\n",
    "    ds   = load_dataset(\"chaojiang06/wiki_auto\", \"manual\", split=\"test[:10]\")\n",
    "    src  = ds[\"normal_sentence\"]\n",
    "    refs = [[r] for r in ds[\"simple_sentence\"]]\n",
    "\n",
    "    coedit_preds  = coedit_edit(src)\n",
    "    mistral_preds = mistral_edit(src)\n",
    "\n",
    "    print(\"-- COEDIT metrics --\")\n",
    "    print(evaluate_outputs(coedit_preds, src, refs))\n",
    "\n",
    "    print(\"\\n-- MISTRAL metrics --\")\n",
    "    print(evaluate_outputs(mistral_preds, src, refs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPA3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
