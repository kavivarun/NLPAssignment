{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2c37302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79168d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ----------------------------\n",
    "def preprocess_texts(texts):\n",
    "    return [\" \".join(t.strip().split()) for t in texts]\n",
    "\n",
    "def add_control_token(texts, style_model, token=\"<simplify>\"):\n",
    "    \"\"\"\n",
    "    Conditionally add a control token before texts only for the specified style model.\n",
    "    \"\"\"\n",
    "    if style_model == \"google/flan-t5-base\":\n",
    "        return [f\"{token} {t}\" for t in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f01489a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# CONFIGURATION & MODELS\n",
    "# ----------------------------\n",
    "GRAMMAR_MODELS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "STYLE_MODELS = [\n",
    "    \"google/flan-t5-base\",\n",
    "    \"sshleifer/distilbart-cnn-12-6\"\n",
    "]\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "BATCH_SIZE = 8\n",
    "PIPELINE_MAX_LENGTH = 256\n",
    "DATASET_SUBSET_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dafa7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LOAD STYLE DATASET (WikiAuto-Manual)\n",
    "# ----------------------------\n",
    "dataset = load_dataset(\"chaojiang06/wiki_auto\", \"manual\")\n",
    "\n",
    "# Randomly sample 100 items from the test split for evaluation\n",
    "test_split_full = dataset[\"test\"]\n",
    "test_split = test_split_full.shuffle(seed=42).select(range(min(DATASET_SUBSET_LENGTH, len(test_split_full))))\n",
    "\n",
    "if 'normal_sentence' in test_split.column_names and 'simple_sentence' in test_split.column_names:\n",
    "    sources = test_split['normal_sentence']\n",
    "    references = [[r] for r in test_split['simple_sentence']]\n",
    "else:\n",
    "    raise KeyError(f\"Expected 'normal_sentence' and 'simple_sentence' in dataset columns: {test_split.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6307c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23ba024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# BUILD INFERENCE PIPELINES\n",
    "# ----------------------------\n",
    "grammar_pipes = {m: pipeline(\"text2text-generation\", model=m, device=DEVICE) for m in GRAMMAR_MODELS}\n",
    "style_pipes   = {m: pipeline(\"text2text-generation\", model=m, device=DEVICE) for m in STYLE_MODELS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7895271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LOAD METRICS\n",
    "# ----------------------------\n",
    "bleu       = evaluate.load(\"bleu\")\n",
    "bertscore  = evaluate.load(\"bertscore\")\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "sari       = evaluate.load(\"sari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f596c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# EVALUATION FUNCTIONS\n",
    "# ----------------------------\n",
    "def evaluate_combination(gm, sm, sources, references):\n",
    "    # Preprocess and correct grammar\n",
    "    clean_src = preprocess_texts(sources)\n",
    "    \n",
    "    # 1️Grammar correction with progress bar\n",
    "    g_texts = []\n",
    "    for idx in tqdm(range(0, len(clean_src), BATCH_SIZE),\n",
    "                    desc=f\"Grammar {gm}\", leave=False):\n",
    "        batch_src = clean_src[idx:idx + BATCH_SIZE]\n",
    "        outs = grammar_pipes[gm](batch_src,\n",
    "                                max_length=PIPELINE_MAX_LENGTH,\n",
    "                                batch_size=BATCH_SIZE)\n",
    "        g_texts.extend([o['generated_text'] for o in outs])\n",
    "    g_clean = preprocess_texts(g_texts)\n",
    "    print(\"Grammar done!\")\n",
    "    \n",
    "    # Style simplification with progress bar\n",
    "    preds = []\n",
    "    for idx in tqdm(range(0, len(g_clean), BATCH_SIZE),\n",
    "                    desc=f\"Style {sm}\", leave=False):\n",
    "        batch_in = add_control_token(g_clean[idx:idx + BATCH_SIZE], sm)\n",
    "        outs = style_pipes[sm](batch_in,\n",
    "                               max_length=PIPELINE_MAX_LENGTH,\n",
    "                               batch_size=BATCH_SIZE)\n",
    "        preds.extend([o['generated_text'] for o in outs])\n",
    "    print(\"Styling done!\")\n",
    "\n",
    "    # Compute metrics\n",
    "    sari_score = sari.compute(sources=clean_src, predictions=preds, references=references)[\"sari\"]\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"bleu\"]\n",
    "    bert_res   = bertscore.compute(predictions=preds, references=[r[0] for r in references], lang=\"en\")\n",
    "    bert_f1    = np.mean(bert_res['f1'])\n",
    "    \n",
    "    # Readability metrics via textstat, averaged over all outputs\n",
    "    fkgl_vals = [textstat.flesch_kincaid_grade(p) for p in preds]\n",
    "    fre_vals  = [textstat.flesch_reading_ease(p) for p in preds]\n",
    "    fkgl       = sum(fkgl_vals) / len(fkgl_vals)\n",
    "    fre        = sum(fre_vals) / len(fre_vals)\n",
    "    \n",
    "    ppl_res    = perplexity.compute(model_id=\"gpt2\", predictions=preds)\n",
    "    ppl        = np.mean(ppl_res.get('perplexities', ppl_res))\n",
    "    return {\n",
    "        'grammar_model': gm,\n",
    "        'style_model': sm,\n",
    "        'sari': sari_score,\n",
    "        'bleu': bleu_score,\n",
    "        'bert_f1': bert_f1,\n",
    "        'fkgl': fkgl,\n",
    "        'flesch': fre,\n",
    "        'perplexity': ppl\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc972491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(sources, references):\n",
    "    results = []\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        for sm in STYLE_MODELS:\n",
    "            res = evaluate_combination(gm, sm, sources, references)\n",
    "            results.append(res)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e55e6da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 63/63 [04:04<00:00,  3.88s/it]\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:09<00:00,  1.10s/it]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:45<00:00,  2.63s/it]\n",
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styling done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:10<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# RUN EVALUATION\n",
    "# ----------------------------\n",
    "df = evaluate_all(sources, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e14ccd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          grammar_model                    style_model       sari      bleu  \\\n",
      "0  google/flan-t5-small            google/flan-t5-base  43.714773  0.005583   \n",
      "1  google/flan-t5-small  sshleifer/distilbart-cnn-12-6  42.296866  0.003587   \n",
      "2    facebook/bart-base            google/flan-t5-base  40.245766  0.006873   \n",
      "3    facebook/bart-base  sshleifer/distilbart-cnn-12-6  35.818627  0.004208   \n",
      "\n",
      "    bert_f1     fkgl    flesch  perplexity  \n",
      "0  0.832651  11.5476  42.58575  180.930183  \n",
      "1  0.826500   7.8150  66.08709   27.124497  \n",
      "2  0.825945  13.1190  27.05256  365.127896  \n",
      "3  0.827489   9.3895  59.00918   34.519559  \n"
     ]
    }
   ],
   "source": [
    "# Sort and display\n",
    "df = df.sort_values('sari', ascending=False).reset_index(drop=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c795145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to c:\\Users\\jonat\\Documents\\UTS\\2025 Autumn Sem\\42850 NLP\\Assignment 3\\NLPAssignment\\Jonathan Workspace\\pipeline_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# SAVE RESULTS\n",
    "# ----------------------------\n",
    "csv_path = os.path.join(os.getcwd(), 'pipeline_evaluation_results.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PLOT RESULTS\n",
    "# ----------------------------\n",
    "pivot_sari = df.pivot(index='grammar_model', columns='style_model', values='sari')\n",
    "pivot_bleu = df.pivot(index='grammar_model', columns='style_model', values='bleu')\n",
    "labels = pivot_sari.index.tolist()\n",
    "x = np.arange(len(labels))\n",
    "width = 0.8 / len(STYLE_MODELS)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i, sm in enumerate(pivot_sari.columns):\n",
    "    plt.bar(x + i*width, pivot_sari[sm], width, label=sm)\n",
    "plt.xticks(x + width*(len(STYLE_MODELS)-1)/2, labels, rotation=45)\n",
    "plt.ylabel('SARI')\n",
    "plt.title('SARI by Pipeline Combo')\n",
    "plt.legend(title='Style Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i, sm in enumerate(pivot_bleu.columns):\n",
    "    plt.bar(x + i*width, pivot_bleu[sm], width, label=sm)\n",
    "plt.xticks(x + width*(len(STYLE_MODELS)-1)/2, labels, rotation=45)\n",
    "plt.ylabel('BLEU')\n",
    "plt.title('BLEU by Pipeline Combo')\n",
    "plt.legend(title='Style Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0211e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# HUMAN‑EVAL CSV GENERATOR\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "def generate_human_eval_csv(input_csv: str, text_column: str, output_csv: str):\n",
    "    \"\"\"Read a CSV, run every Grammar→Style combo on each row, and write a new CSV\n",
    "    with side‑by‑side outputs ready for human scoring.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_csv : str\n",
    "        Path to the source CSV that contains raw presentation sentences.\n",
    "    text_column : str\n",
    "        Column name that holds the text to be rewritten.\n",
    "    output_csv : str\n",
    "        Destination path for the human‑evaluation CSV.\n",
    "    \"\"\"\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "    if text_column not in df_in.columns:\n",
    "        raise KeyError(f\"Column '{text_column}' not found in {input_csv}.\")\n",
    "\n",
    "    raw_texts = df_in[text_column].astype(str).tolist()\n",
    "    # Pre-clean once\n",
    "    clean_texts = preprocess_texts(raw_texts)\n",
    "\n",
    "    # For each grammar model, pre‑compute corrected text\n",
    "    corrected_cache = {}\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        g_texts = []\n",
    "        for idx in tqdm(range(0, len(clean_texts), BATCH_SIZE),\n",
    "                        desc=f\"Grammar pass ({gm})\", leave=False):\n",
    "            batch_src = clean_texts[idx:idx + BATCH_SIZE]\n",
    "            outs = grammar_pipes[gm](batch_src, max_length=PIPELINE_MAX_LENGTH,\n",
    "                                     batch_size=BATCH_SIZE)\n",
    "            g_texts.extend([o['generated_text'] for o in outs])\n",
    "        corrected_cache[gm] = preprocess_texts(g_texts)\n",
    "\n",
    "    # Build new DataFrame starting with the original text\n",
    "    df_out = pd.DataFrame({text_column: raw_texts})\n",
    "\n",
    "    # For every Grammar→Style combo, add a column\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        g_clean = corrected_cache[gm]\n",
    "        for sm in STYLE_MODELS:\n",
    "            col_name = f\"{gm}__{sm}\"\n",
    "            preds = []\n",
    "            for idx in tqdm(range(0, len(g_clean), BATCH_SIZE),\n",
    "                            desc=f\"Style pass ({gm}->{sm})\", leave=False):\n",
    "                batch_in = add_control_token(g_clean[idx:idx+BATCH_SIZE], sm)\n",
    "                outs = style_pipes[sm](batch_in, max_length=PIPELINE_MAX_LENGTH,\n",
    "                                       batch_size=BATCH_SIZE)\n",
    "                preds.extend([o['generated_text'] for o in outs])\n",
    "            df_out[col_name] = preds\n",
    "\n",
    "    # Optional blank columns for human scores\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        for sm in STYLE_MODELS:\n",
    "            df_out[f\"score_{gm}__{sm}\"] = \"\"  # empty cell to fill manually\n",
    "\n",
    "    df_out.to_csv(output_csv, index=False)\n",
    "    print(f\"Human‑eval CSV saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100da13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_human_eval_csv('input_transcripts.csv', 'sentence', 'human_eval_outputs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
