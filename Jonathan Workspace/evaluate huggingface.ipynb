{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c37302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonat\\anaconda3\\envs\\NLP3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79168d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ----------------------------\n",
    "def preprocess_texts(texts):\n",
    "    return [\" \".join(t.strip().split()) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01489a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# CONFIGURATION & MODELS\n",
    "# ----------------------------\n",
    "GRAMMAR_MODELS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "STYLE_MODELS = [\n",
    "    # \"google/flan-t5-base\",\n",
    "    \"sshleifer/distilbart-cnn-12-6\",\n",
    "    \"rajistics/informal_formal_style_transfer\"\n",
    "]\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "BATCH_SIZE = 8\n",
    "PIPELINE_MAX_LENGTH = 256\n",
    "DATASET_SUBSET_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafa7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LOAD STYLE DATASET (WikiAuto-Manual)\n",
    "# ----------------------------\n",
    "dataset = load_dataset(\"chaojiang06/wiki_auto\", \"manual\")\n",
    "\n",
    "# Randomly sample 100 items from the test split for evaluation\n",
    "test_split_full = dataset[\"test\"]\n",
    "test_split = test_split_full.shuffle(seed=42).select(range(min(DATASET_SUBSET_LENGTH, len(test_split_full))))\n",
    "\n",
    "if 'normal_sentence' in test_split.column_names and 'simple_sentence' in test_split.column_names:\n",
    "    sources = test_split['normal_sentence']\n",
    "    references = [[r] for r in test_split['simple_sentence']]\n",
    "else:\n",
    "    raise KeyError(f\"Expected 'normal_sentence' and 'simple_sentence' in dataset columns: {test_split.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6307c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ba024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\jonat\\anaconda3\\envs\\NLP3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jonat\\.cache\\huggingface\\hub\\models--rajistics--informal_formal_style_transfer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# BUILD INFERENCE PIPELINES\n",
    "# ----------------------------\n",
    "# grammar_pipes = {m: pipeline(\"text2text-generation\", model=m, device=DEVICE) for m in GRAMMAR_MODELS}\n",
    "# style_pipes   = {m: pipeline(\"text2text-generation\", model=m, device=DEVICE) for m in STYLE_MODELS}\n",
    "\n",
    "grammar_pipes = {\n",
    "    m: pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=m,\n",
    "        device=DEVICE,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        max_length=PIPELINE_MAX_LENGTH\n",
    "    )\n",
    "    for m in GRAMMAR_MODELS\n",
    "}\n",
    "\n",
    "style_pipes = {\n",
    "    m: pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=m,\n",
    "        device=DEVICE,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.8,\n",
    "        num_beams=4,\n",
    "        max_length=PIPELINE_MAX_LENGTH / 2\n",
    "    )\n",
    "    for m in STYLE_MODELS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7895271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LOAD METRICS\n",
    "# ----------------------------\n",
    "bleu       = evaluate.load(\"bleu\")\n",
    "bertscore  = evaluate.load(\"bertscore\")\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "sari       = evaluate.load(\"sari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "511257f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipe_with_prefix(\n",
    "    pipe,\n",
    "    texts: list[str],\n",
    "    model_name: str,\n",
    "    is_grammar: bool,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_len: int = PIPELINE_MAX_LENGTH,\n",
    "):\n",
    "    \n",
    "    # Determine model family → choose prefixes\n",
    "    t5 = \"t5\" in model_name.lower()\n",
    "    if t5:\n",
    "        gram_pref  = \"Fix all grammar and spelling errors> \"\n",
    "        style_pref = \"Paraphase and Simplify> \"\n",
    "    else:\n",
    "        gram_pref  = \"Fix all grammar and spelling errors> \"\n",
    "        style_pref = \"Paraphase and Simplify> \"\n",
    "\n",
    "    prefix = gram_pref if is_grammar else style_pref\n",
    "    strip_prefixes = (gram_pref, style_pref, \"Paraphrase: \", \"Grammar correction: \")\n",
    "\n",
    "    outputs_all = []\n",
    "    \n",
    "    for i in tqdm(\n",
    "        range(0, len(texts), batch_size),\n",
    "        desc=f\"{'Grammar' if is_grammar else 'Style'} pass ({model_name})\",\n",
    "        leave=False,\n",
    "    ):\n",
    "        chunk = texts[i : i + batch_size]\n",
    "\n",
    "        # 1) add prefix\n",
    "        inp_with_pref = [prefix + txt for txt in chunk]\n",
    "\n",
    "        # 2) pipeline call\n",
    "        outs = pipe(\n",
    "            inp_with_pref,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # 3) decode & strip prefixes\n",
    "        for out in outs:\n",
    "            decoded = out[\"generated_text\"].strip()\n",
    "            for p in strip_prefixes:\n",
    "                if decoded.startswith(p):\n",
    "                    decoded = decoded[len(p):].lstrip()\n",
    "            outputs_all.append(decoded)\n",
    "\n",
    "    return outputs_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_df(\n",
    "    df_out: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    src_clean,\n",
    "    references\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # iterate over every system column (skip original + score columns)\n",
    "    for col in df_out.columns:\n",
    "        if col == text_column or col.startswith(\"score_\"):\n",
    "            continue\n",
    "\n",
    "        preds = df_out[col].astype(str).tolist()\n",
    "\n",
    "        # ---- metrics -------------------------------------------------------\n",
    "        sari_score = sari.compute(\n",
    "            sources=src_clean,\n",
    "            predictions=preds,\n",
    "            references=references\n",
    "        )[\"sari\"]\n",
    "\n",
    "        bleu_score = bleu.compute(\n",
    "            predictions=preds,\n",
    "            references=references\n",
    "        )[\"bleu\"]\n",
    "\n",
    "        bert_res  = bertscore.compute(\n",
    "            predictions=preds,\n",
    "            references=[r[0] for r in references],\n",
    "            lang=\"en\"\n",
    "        )\n",
    "        bert_f1   = float(np.mean(bert_res[\"f1\"]))\n",
    "\n",
    "        fkgl_vals = [textstat.flesch_kincaid_grade(p) for p in preds]\n",
    "        fre_vals  = [textstat.flesch_reading_ease(p) for p in preds]\n",
    "        fkgl      = float(np.mean(fkgl_vals))\n",
    "        fre       = float(np.mean(fre_vals))\n",
    "\n",
    "        ppl_vals  = perplexity.compute(\n",
    "            model_id=\"gpt2\",\n",
    "            predictions=preds\n",
    "        ).get(\"perplexities\")\n",
    "        ppl       = float(np.mean(ppl_vals))\n",
    "\n",
    "        # split column name back into grammar / style\n",
    "        gm, sm = col.split(\"__\")\n",
    "\n",
    "        results.append({\n",
    "            \"grammar_model\": gm,\n",
    "            \"style_model\":   sm,\n",
    "            \"sari\":  sari_score,\n",
    "            \"bleu\":  bleu_score,\n",
    "            \"bert_f1\": bert_f1,\n",
    "            \"fkgl\":  fkgl,\n",
    "            \"flesch\": fre,\n",
    "            \"perplexity\": ppl\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"sari\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f596c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# EVALUATION FUNCTIONS\n",
    "# ----------------------------\n",
    "def evaluate_models(sources, references):\n",
    "    # Preprocess and correct grammar\n",
    "    text_column = \"sentences\"\n",
    "    clean_texts = preprocess_texts(sources)\n",
    "    corrected_cache = {}\n",
    "    df_out      = pd.DataFrame({text_column: clean_texts})\n",
    "\n",
    "    # --- Grammar stage ----------------------------------------------------------\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        print(f\"Grammar pass – {gm}\")\n",
    "        g_pipe  = grammar_pipes[gm]\n",
    "        g_fixed = run_pipe_with_prefix(\n",
    "            g_pipe, clean_texts, gm, is_grammar=True\n",
    "        )\n",
    "        corrected_cache[gm] = preprocess_texts(g_fixed)\n",
    "    print(\"Grammar done!\")\n",
    "\n",
    "    # --- Style stage ------------------------------------------------------------\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        g_clean = corrected_cache[gm]\n",
    "        for sm in STYLE_MODELS:\n",
    "            col_name = f\"{gm.split('/')[-1]}__{sm.split('/')[-1]}\"\n",
    "            print(f\"Style pass – {gm} → {sm}\")\n",
    "            s_pipe = style_pipes[sm]\n",
    "            preds  = run_pipe_with_prefix(\n",
    "                s_pipe, g_clean, sm, is_grammar=False\n",
    "            )\n",
    "            df_out[col_name] = preds\n",
    "    print(\"Styling done!\")\n",
    "\n",
    "    return build_eval_df(df_out, text_column, clean_texts, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e6da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar pass – google/flan-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar pass – facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar done!\n",
      "Style pass – google/flan-t5-small → sshleifer/distilbart-cnn-12-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – google/flan-t5-small → rajistics/informal_formal_style_transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – facebook/bart-base → sshleifer/distilbart-cnn-12-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style pass (sshleifer/distilbart-cnn-12-6):  84%|████████▍ | 32/38 [05:22<01:10, 11.78s/it]"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# RUN EVALUATION\n",
    "# ----------------------------\n",
    "df = evaluate_models(sources, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e14ccd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grammar_model          style_model       sari      bleu   bert_f1  \\\n",
      "0  flan-t5-small  distilbart-cnn-12-6  35.846313  0.004268  0.827771   \n",
      "1      bart-base  distilbart-cnn-12-6  35.844019  0.002804  0.827132   \n",
      "2      bart-base      bart-paraphrase  34.876300  0.000000  0.841546   \n",
      "3  flan-t5-small      bart-paraphrase  34.846586  0.000000  0.841755   \n",
      "\n",
      "        fkgl     flesch  perplexity  \n",
      "0   9.382333  57.733933   43.264453  \n",
      "1   9.524333  57.252900   43.018335  \n",
      "2  12.062333  47.044800  108.447141  \n",
      "3  11.978333  47.335033  111.588177  \n"
     ]
    }
   ],
   "source": [
    "# Sort and display\n",
    "df = df.sort_values('sari', ascending=False).reset_index(drop=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c795145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to c:\\Users\\jonat\\Documents\\UTS\\2025 Autumn Sem\\42850 NLP\\Assignment 3\\NLPAssignment\\Jonathan Workspace\\pipeline_evaluation_results_huggingface.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# SAVE RESULTS\n",
    "# ----------------------------\n",
    "csv_path = os.path.join(os.getcwd(), 'pipeline_evaluation_results_huggingface.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PLOT RESULTS\n",
    "# ----------------------------\n",
    "pivot_sari = df.pivot(index='grammar_model', columns='style_model', values='sari')\n",
    "pivot_bleu = df.pivot(index='grammar_model', columns='style_model', values='bleu')\n",
    "labels = pivot_sari.index.tolist()\n",
    "x = np.arange(len(labels))\n",
    "width = 0.8 / len(STYLE_MODELS)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i, sm in enumerate(pivot_sari.columns):\n",
    "    plt.bar(x + i*width, pivot_sari[sm], width, label=sm)\n",
    "plt.xticks(x + width*(len(STYLE_MODELS)-1)/2, labels, rotation=45)\n",
    "plt.ylabel('SARI')\n",
    "plt.title('SARI by Pipeline Combo')\n",
    "plt.legend(title='Style Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for i, sm in enumerate(pivot_bleu.columns):\n",
    "    plt.bar(x + i*width, pivot_bleu[sm], width, label=sm)\n",
    "plt.xticks(x + width*(len(STYLE_MODELS)-1)/2, labels, rotation=45)\n",
    "plt.ylabel('BLEU')\n",
    "plt.title('BLEU by Pipeline Combo')\n",
    "plt.legend(title='Style Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0211e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# HUMAN‑EVAL CSV GENERATOR\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "def generate_human_eval_csv(input_csv: str, text_column: str, output_csv: str):\n",
    "    \"\"\"Read a CSV, run every Grammar→Style combo on each row, and write a new CSV\n",
    "    with side‑by‑side outputs ready for human scoring.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_csv : str\n",
    "        Path to the source CSV that contains raw presentation sentences.\n",
    "    text_column : str\n",
    "        Column name that holds the text to be rewritten.\n",
    "    output_csv : str\n",
    "        Destination path for the human‑evaluation CSV.\n",
    "    \"\"\"\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "    if text_column not in df_in.columns:\n",
    "        raise KeyError(f\"Column '{text_column}' not found in {input_csv}.\")\n",
    "\n",
    "    raw_texts = df_in[text_column].astype(str).tolist()\n",
    "    # Pre-clean once\n",
    "    clean_texts = preprocess_texts(raw_texts)\n",
    "    df_out      = pd.DataFrame({text_column: raw_texts})\n",
    "    corrected_cache = {}\n",
    "\n",
    "    # --- Grammar stage ----------------------------------------------------------\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        print(f\"Grammar pass – {gm}\")\n",
    "        g_pipe  = grammar_pipes[gm]\n",
    "        g_fixed = run_pipe_with_prefix(\n",
    "            g_pipe, clean_texts, gm, is_grammar=True\n",
    "        )\n",
    "        corrected_cache[gm] = preprocess_texts(g_fixed)\n",
    "\n",
    "    # --- Style stage ------------------------------------------------------------\n",
    "    for gm in GRAMMAR_MODELS:\n",
    "        g_clean = corrected_cache[gm]\n",
    "        for sm in STYLE_MODELS:\n",
    "            col_name = f\"{gm.split('/')[-1]}__{sm.split('/')[-1]}\"\n",
    "            print(f\"Style pass – {gm} → {sm}\")\n",
    "            s_pipe = style_pipes[sm]\n",
    "            preds  = run_pipe_with_prefix(\n",
    "                s_pipe, g_clean, sm, is_grammar=False\n",
    "            )\n",
    "            df_out[col_name] = preds\n",
    "\n",
    "    # Optional blank columns for human scores\n",
    "    # for gm in GRAMMAR_MODELS:\n",
    "    #     for sm in STYLE_MODELS:\n",
    "    #         df_out[f\"score_{gm}__{sm}\"] = \"\"  # empty cell to fill manually\n",
    "\n",
    "    df_out.to_csv(output_csv, index=False)\n",
    "    print(f\"Human‑eval CSV saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100da13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar pass – google/flan-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar pass – facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – google/flan-t5-small → sshleifer/distilbart-cnn-12-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – google/flan-t5-small → rajistics/informal_formal_style_transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – facebook/bart-base → sshleifer/distilbart-cnn-12-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style pass – facebook/bart-base → rajistics/informal_formal_style_transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human‑eval CSV saved to human_eval_outputs10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "generate_human_eval_csv('verbose_samples.csv', 'sentence', 'human_eval_outputs10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1264dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_human_eval(input_csv: str, output_csv: str, mapping_csv: str):\n",
    "    \"\"\"\n",
    "    Randomize columns A–D per row and prepare a CSV for blind human evaluation.\n",
    "\n",
    "    Params:\n",
    "    - input_csv: CSV with columns: sentence, A, B, C, D\n",
    "    - output_csv: randomized file for human raters\n",
    "    - mapping_csv: stores the randomized column origin per row (for decoding later)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) < 5:\n",
    "        raise ValueError(f\"Input CSV must have at least 5 columns, found {len(cols)}: {cols}\")\n",
    "    sentence_col = cols[0]\n",
    "    model_cols = cols[1:5]\n",
    "    \n",
    "    randomized_rows = []\n",
    "    mappings = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        options = [row[c] for c in model_cols]\n",
    "        zipped = list(zip(model_cols, options))\n",
    "        random.shuffle(zipped)\n",
    "\n",
    "        labels, shuffled_outputs = zip(*zipped)\n",
    "        randomized_rows.append({\n",
    "            'sentence': row[sentence_col],\n",
    "            'option_1': shuffled_outputs[0],\n",
    "            'option_2': shuffled_outputs[1],\n",
    "            'option_3': shuffled_outputs[2],\n",
    "            'option_4': shuffled_outputs[3],\n",
    "            'chosen': ''  # to be filled by rater (1–4)\n",
    "        })\n",
    "\n",
    "        mappings.append({\n",
    "            'row_id': len(mappings),\n",
    "            'option_1': labels[0],\n",
    "            'option_2': labels[1],\n",
    "            'option_3': labels[2],\n",
    "            'option_4': labels[3]\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(randomized_rows).to_csv(output_csv, index=False)\n",
    "    pd.DataFrame(mappings).to_csv(mapping_csv, index=False)\n",
    "    print(f\"Human evaluation CSV saved to: {output_csv}\")\n",
    "    print(f\"Mapping CSV saved to: {mapping_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eced3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_human_eval(eval_csv: str, mapping_csv: str):\n",
    "    \"\"\"\n",
    "    Read the human evaluation and determine which model (A/B/C/D) was preferred most.\n",
    "\n",
    "    Params:\n",
    "    - eval_csv: the CSV with human ratings (option_1, ..., chosen)\n",
    "    - mapping_csv: the shuffled mapping created earlier\n",
    "    \"\"\"\n",
    "    df_eval = pd.read_csv(eval_csv)\n",
    "    df_map = pd.read_csv(mapping_csv)\n",
    "\n",
    "    vote_counter = {'A': 0, 'B': 0, 'C': 0, 'D': 0}\n",
    "    total_votes = 0\n",
    "\n",
    "    for idx, row in df_eval.iterrows():\n",
    "        chosen = row.get('chosen')\n",
    "        if pd.isna(chosen) or str(chosen).strip() not in {'1', '2', '3', '4'}:\n",
    "            continue  # skip unscored rows\n",
    "\n",
    "        chosen_idx = int(chosen.strip())\n",
    "        col_origin = df_map.loc[idx, f'option_{chosen_idx}']\n",
    "        vote_counter[col_origin] += 1\n",
    "        total_votes += 1\n",
    "\n",
    "    print(\"Human Evaluation Results (model preference counts):\")\n",
    "    for model, count in vote_counter.items():\n",
    "        pct = (count / total_votes * 100) if total_votes else 0\n",
    "        print(f\"  {model}: {count} votes ({pct:.1f}%)\")\n",
    "    \n",
    "    best = max(vote_counter, key=vote_counter.get)\n",
    "    print(f\"\\nBest performing model (by human preference): {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb148aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create blinded evaluation sheet\n",
    "prepare_human_eval(\"results.csv\", \"for_human_rating.csv\", \"shuffle_map.csv\")\n",
    "\n",
    "# Step 2 (after rating): Analyze winner\n",
    "analyze_human_eval(\"for_human_rating.csv\", \"shuffle_map.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
